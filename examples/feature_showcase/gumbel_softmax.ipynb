{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gumbel-Softmax - New feature\n",
    "This notebook showcases a new feature introduced in version 0.6, Gumbel-Softmax activations!\n",
    "\n",
    "**Structure of the notebook:**\n",
    "\n",
    "1. A quick recap on categorical feature synthesis\n",
    "2. Softmax and the Gumbel-Softmax activation\n",
    "3. Synthesized categorical features (before)\n",
    "4. Synthesized categorical features (now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A quick recap on categorical feature synthesis\n",
    "Before synthesizing we typically preprocess our features. In the case of categorical features, one-hot encodings are frequently used in order to transform discrete features into sparse blocks of 1's and 0's. Converting symbolic inputs like categorical features to sparse arrays allows neural network (NN) models to handle the data similarly to very different feature formats like numerical continuous features.\n",
    "\n",
    "An example:\n",
    "* Before one-hot encoding:\n",
    "<style>\n",
    "th {\n",
    "  padding-top: 5px;\n",
    "  padding-right: 10px;\n",
    "  padding-bottom: 5px;\n",
    "  padding-left: 10px;\n",
    "}\n",
    "</style>\n",
    "\n",
    "| ID | Gender | AgeRange |\n",
    "| :------------: | :-------:  | :-------:  |\n",
    "| 1 | Male | 20-29 |\n",
    "| 2 | Female | 10-19 |\n",
    "\n",
    "* After one-hot encoding:\n",
    "<style>\n",
    "th {\n",
    "  padding-top: 5px;\n",
    "  padding-right: 10px;\n",
    "  padding-bottom: 5px;\n",
    "  padding-left: 10px;\n",
    "}\n",
    "</style>\n",
    "| ID | Gender_Male | Gender_Female | AgeRange_10-19 | AgeRange_20-29 |\n",
    "| :------------: | :-------:  | :-------:  | :-------:  | :-------:  |\n",
    "| 1 | 1 | 0 | 0 | 1 |\n",
    "| 2 | 0 | 1 | 1 | 0 |\n",
    "\n",
    "GANs attempt to synthesize these sparse distributions as they appear on real data. However, despite the input categorical features having a sparse format, NN classifiers learn __[logits](https://en.wikipedia.org/wiki/Logit)__, non-normalized probability distributions, for each class represented in the one-hot encoded input. Without a final layer (for convention lets call it an activation layer) to handle this output, you might get a float output looking something like this:\n",
    "<style>\n",
    "th {\n",
    "  padding-top: 5px;\n",
    "  padding-right: 10px;\n",
    "  padding-bottom: 5px;\n",
    "  padding-left: 10px;\n",
    "}\n",
    "</style>\n",
    "| ID | Gender_Male | Gender_Female | AgeRange_10-19 | AgeRange_20-29 |\n",
    "| :------------: | :-------:  | :-------:  | :-------:  | :-------:  |\n",
    "| 1 | 0.867 | 0.622 | -0.155 | 0.855 |\n",
    "| 2 | 0.032 | 1.045 | 0.901 | -0.122 |\n",
    "\n",
    "This looks messy; leaves you with the job of inferring a sensible output (p.e. use the class with highest activation) and also is a potential flag for a GAN discriminator to identify fake samples.\n",
    "\n",
    "Let's see what Gumbel-Softmax can do about it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax and the Gumbel-Softmax activation\n",
    "Softmax is a differentiable family of functions that map an array of logits to probabilities, i.e. values are bounded in the range $[0, 1]$ and sum to 1.\n",
    "These are often used for turning logits into probability distributions from which we can sample. However these samples can't help us in gradient descent model learning because they are obtained from a random process (no relation with the model's parameters).\n",
    "\n",
    "The Gumbel-Softmax is a special kind of Softmax function. It works like a continuous approximation of Softmax. Instead of using logits directly __[Gumbel distribution](https://en.wikipedia.org/wiki/Gumbel_distribution)__ noise is added before the softmax operation so that our model is outputting a combination from a deterministic component, parameterized by the mean and the variance of the categorical distribution, and a stochastic component, the Gumbel noise, which is just helping us sample without adding bias to the process.\n",
    "\n",
    "A temperature parameter, usually called tau or lambda and defined in $]0, inf[$ is used to tune this distribution between the true categorical distribution and a uniform distribution respectively. This parameter is usually kept close to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
